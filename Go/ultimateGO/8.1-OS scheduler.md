\- All right, we're gonna start talking about concurrency, and what I wanna do right now is give you the mechanical sympathies of the Go schedule. I want you to get an understanding of where the sympathies are and why Go is gonna be able to do the things that it can for us, which is really reduce a lot of cognitive load that we used to have when we were writing multi-threaded software directly with operating system threads. But Go can't take off all of the cognitive load. In fact, I don't think there is a single language we could even invent that can do that. We're still always gonna be responsible for understanding our workloads. We're always gonna be responsible for what I call synchronization and orchestration. We're gonna start talking about what these things are. But let's start right from the beginning and get a general representational level understanding of how the operating system schedulers work. Now an operating system scheduler is what we call a preemptive scheduler, which basically means that when the scheduler of the operating system is making a decision about how to schedule threads, we cannot predict what the scheduler's gonna do, especially if all things are equal. They are preemptive, which basically means undeterministic. Now when a program starts up on your operating system, I don't care what operating system it is, Windows, Mac, or Linux, what's gonna happen is the operating system's gonna create a process. I always like to think of that process as a container for the resources that that running program is going to need. This word container's got a lot of baggage around it, so don't be thinking about Docker and all that other stuff just right now. Just think of it as a process is a way of maintaining and managing the resources for that running program, and there's two big key resources, there's lots of them, but there's two big key resources that we are talking about all the time. One is memory, let's remember that process has been given a full memory map based on the 64-bit or 32-bit architecture of memory. Now that process thinks it's real, but we know it's virtual. But there's another thing that that process is given, and we call it a thread. In fact, it's the main thread, and it's the main thread because when the main thread dies, guess what? The process is shut down. Now if a process is like a container for the resources, then I want you to consider the thread to be the path of execution. That's what I like to think of threads as, paths of execution. Remember, you're writing code, and that code turns into machine code, and machine code is always gonna be executed in a linear fashion, right? Do this instruction, that instruction, that instruction, that instruction, even if there's a Jump statement, that instruction says where to jump to next, do this, do this, do this. So it's always this linear execution or this linear path of execution. And it's the job of the thread to manage that linear path of execution. We have what's called an instruction point, or sometimes we see it as PC, the program counter. You'll see that in stat traces. The PC's telling us what that, at the hardware level, what that instruction was that we had to execute next. Okay, so we have these paths of execution, and the schedulers of the operating system don't care about processes. All they really care about are paths of execution. Now, we're gonna keep things simple and clean and representative and we can maintain these visual mental models of things. We don't have to really get complex. From our perspective, a path of execution or thread can be in one of three states. A thread can be executing, or you'll hear me say running. It's actually been placed on a core, and now whatever the next instruction is for that thread, its next program command, that next instruction, whatever it's pointing to, that's the next instruction that's now going to execute. And that path, or that thread, will get to keep executing its instructions until the operating system, in a non-deterministic way, decides that that thread is no longer allowed to run on that core, and then performs a content switch. A content switch is when we have a thread executing on some core and it's pulled off the core and a new thread is placed on the core. Now a thread can only be placed on the core if it's in a runnable state. That's the other state. So we have running, then we have runnable. So the operating system's going to look at all of the threads that are in a runnable state, gonna choose that next one, however that it, it's undeterministic, and now that thread gets to execute its path of execution. Now a content switch in the operating system is very, very expensive, okay? The operating system really doesn't know what these threads are doing. It knows that they're in this now state that they wanna run. They don't really necessarily know what it's doing. So a tremendous amount of information has to be saved out of that core the thread is running on, so when it's placed back on the core, it can be done so in a way that the thread had no idea that it ever really stopped. So this concept of saving states and replacing states is very important and it's very expensive. You remember also that threads have a one meg stack space. That also makes threads very, very expensive. Okay, the third state that a thread can be in is, we'll just call it the waiting state. There are many sub-states when it comes to waiting, that's not important for us. What's important is that the thread is now in a waiting state, and from an execution standpoint, when a thread is in a waiting state, I like to think of it as in the matrix. It's gone. It disappears. It almost, it disappears from view. And it disappears from view until it moves back into a runnable state. There's lots of reasons why a thread could go into a waiting state. It's waiting for something, waiting for something from the operating system. Most likely, we're waiting for disc I/O, network I/O, something that you've done that says, "You know what? "We don't need to schedule this thread right now "because it doesn't want to run. "It's not in that runnable state." So we've got running, runnable, and waiting. These are the three states that the scheduler's gonna be managing all the time, and we've got the content switch. Now, in the early days, it wasn't really a complicated problem. In the early days we had one core. So imagine we've got this one core right here, here it is, this is our execution unit, and let's imagine that we had one core in our piece of hardware and that core, and again, we're gonna keep things very, very, very simple, that core, even today, could only execute one path of execution at a time. 



Even though there's multiple hardware threads in there, from our place here, that core can only execute one operating system or path of execution at a time. 

So if I've got one core, no matter what, we're only gonna be able to execute a single thread. 

Now in the, back in the day, let's say that you had 10,000 of these threads, 10,000 threads or paths of execution that want access to this execution unit. 

Now one of the jobs of the operating system is to make it seem, right, **perceive**, right, that all of these paths of execution or threads are executing at the same time. We know that that's **not** physically possible, because we only have this one core or execution unit. So what the **operating system's scheduler's** gotta do to create this perception that everything's running at the same time is give every one of these threads a **slice of time** on the core. 

Now when we only have this one core, this is not necessarily a very complicated problem. All we've gotta kind of say to ourselves is, "I'd like to create the perception "that all these threads ran "within a certain amount of time." 

So if we take that time and divide it by the number of threads, well then, each thread will get that amount of time to run, come off, run, come off, run, come off, and if any of these threads go from a running state into a waiting state, well, then we'll take, they'll execute less time, which will now give another thread that opportunity to just get on the core quicker. 



And we know operating systems have the idea of **priority** and can prioritize these threads, but we don't need to really go to that level of complexity right now. The idea is that if there's more threads, like 10,000 threads, then each thread's going to really get less time, because we've gotta create this perception that they're all running. But what if there was only a thousand threads, not 10,000, a thousand threads? Then we get the idea that each thread's going to get more time, because there's less threads that we have to worry about scheduling within that amount of time, right? So everything that I've been talking about, and especially everything here in multi-threaded software development, is going to be this idea of **less is more**. We've gotta keep that in our head all the time. Less is more. More threads mean more load on the operating system scheduler, it's gotta make more scheduling decisions, there is more chaos, which means more **scheduling decisions**, more **content switches**, and understand that when a content switch is happening, your paths of execution are not running, right? Operating system code is running to pull this off and to push this back on. We can consider a content switch application **latency** because for that brief amount of time, even if it's just nanoseconds, right, that's nanoseconds of time, clock cycles, where your code is not executing, and it's really, at the end of the day, creating a little bit of chaos for everyone. So less is always more. 



But something interesting happened in **2004**. In 2004, we started seeing multi-core processors become mainstream in the marketplace. In other words, it got to the point where you couldn't buy a server that you were gonna rack in your data center that didn't have multiple cores. Now that sounded really exciting. Wow, I don't have just one execution unit, I now have two, which now means that if I've got two execution units, that means that I can now execute not just one thread at a time, but two threads at a time, which means now I can run threads in parallel. Look, concurrency really just means managing a lot of things at once. When we're in our single-core environment, the operating system is managing concurrency. It's trying to execute all of these threads one at a time on this core. That's concurrency, right? The perception that things are happening all at the same time, but we gotta manage all of these paths of execution. Once we're up to two cores, okay, in this scenario here, we now have what's called parallelism, and that means doing a lot of things at once. We're executing two threads in parallel, right, these paths of execution are happening at the same time. And when this started happening, when we started getting hardware with multiple cores on it, the academics started doing research. And they started identifying that the operating system schedulers didn't know really how to efficiently handle this situation. What they saw were threads in a runnable state, right, with core that was idle for nanoseconds to microseconds at a time and those threads not being scheduled. This is a nightmare if we're really trying to leverage that second core to run faster. 



And there's a lot of complexity that we'll talk about before we're done here about how multi-threaded software, especially on a multi-core processor, can get very complicated very, very fast. But we come back here, and what that meant was, in 2004, the scheduling problem opened back up. We had to come back in and rewrite the schedulers to take advantage and be smarter about our multi-core processors. Now, I wanna show you a little bit of why the scheduler really isn't very complex, at least I consider it. Complex algorithm when it comes to efficiency, especially when the scheduler doesn't really know what the threads are doing. I mean, let's go back to our I7900 core processor here that we were looking at earlier in the class. 



And if you remember, hopefully you haven't skipped to this section. If you have, look, if you've skipped to this section, 'cause all you care about is concurrency, I'm gonna just really beg you to stop and go through this video from the beginning, because I've laid down a huge amount of foundational knowledge so we can have this conversation right now, and you can have it with the full breadth of knowledge that I'm about to go, and so I'm gonna assume already that you've gone through that array section and you've looked at the mechanical sympathies around data. This is what we're doing and if you haven't, I really urge you to stop and go back and look at that stuff. 



So remember, we talked about that we have our **four-core** processor, and here it is, and that these processors, these cores, have an L1 and an L2 cache, right? L1, L2 cache. We also talked about the L3 cache here in main memory. So remember something here. The scheduler's got to be very intelligent about how the hardware works, about how the hardware is laid out, in order for it to be efficient. I mean, processors like this, these cores, we talked about this as well. These cores talk to each other, right? They talk to each other and in order for them top be able to talk to each other, right, there are pathways, physical pathways there, and we can notice here that these two cores are closer together than these two cores, right? These two cores are closer. They can talk to each other much faster than these two cores can. The hardware wants to know about this stuff, and what gets really interesting to me is when we start getting into these, like, 24-core processors, we can't use these rings anymore, right? The distance between any two cores could be quite great, and the hardware is starting to deal with these matrices, where we're putting these cores, right, on a matrix here to try to reduce the largest path between any two cores. These cores have to talk to each other. Throwing more cores at a problem doesn't mean it's going to get faster if you're not mechanically sympathetic with the way caching systems and processors work. 



But, the processor, the schedulers have to know all of this stuff. So I want you to think about this for a second. Let's imagine that you started a process, and it starts on core 1, so that means that the main thread for this process, now the operating system says, "*We're going to execute this thread here on core 1.*" So as this path of execution is going, it's going to need data. **Everything's a data problem**. We've talked about that. And so what happens is the cache lines start coming in to L1, L2, and L3, right? They're all coming in from our main memory, right? This application is working with data, it's coming in. Now, this becomes interesting. 



Let's say that the main thread decides to create a second thread, thread two. We're gonna start a second path of execution here with the idea that I'm on a multi-core processor, we should be able to run these and **parallel**. But, we're probably maybe on a very, very busy machine, and you remember our caching systems are a way of reducing those cache line and TLB misses here within our processors. So this is always an interesting question if you were to be writing an operating system scheduler. Should the operating system scheduler, knowing that these cores are busy already, okay, do we know they were, should the operating system scheduler consider to pull the, or to stop, right, put into a runnable state again, the main thread so thread two can start running on core one with the idea that the chances that the data that the main thread is using is going to be the same as thread two, right? A family of threads are probably working with the same data, and since the data's already cached in C1, wouldn't it be more efficient to say, "Okay, you know what? "Let's pull you off of this core. "Let's put you back on this core here, "because the data you need is already there." (chuckles) This is complicated. But what if this core 2 right now is about to become idle? Wouldn't it be better to run thread two on core two and get that executing, even though we're gonna have to probably take some cache line misses, even though we're gonna be duplicating data across these particular cores from a caching perspective, right? Wouldn't that be better, too? I'm not asking you to answer this question. What I'm trying to tell you is these are decisions that have to be made. And decisions have to be made about how close any particular two cores are, too. This is cool stuff. But I will tell you this. If a core is idle, it's gonna be used. We're not gonna wait there, right? We wanna get things executing, if it means we have to bring in that data. But what we wanna do, what the operating system is gonna try to do, is create what are called run queues. So any time a thread is in a runnable state, what we're gonna end up doing is putting these threads in a run queue. And we could have these run queues at a core level, we could have run queues at a processor, there's a whole tree of hierarchy in these run queues. And the operating system is taking these threads and they're placing them in these run queues so the operating system can eventually get these threads in as efficient manner as possible executing. And the Linux operating system is brilliant at this stuff, even on our multi-core processors today. All of this is going on underneath, we don't have to necessarily worry about it, except we do have to be mechanically sympathetic with the operating system because if we're not, we're not really going to get the performance we want and we could be not only just thrashing through memory, but thrashing through content which is of latency. None of that is really, really gonna help us. So we have two responsibilities here as a multi-threaded software developer. We've got to understand our workloads, we've gotta understand when to use synchronization and orchestration. When I talk about understanding our workload, what I'm talking about is there's really two types of workloads that we have to think about. There's **CPU-bound workloads** and there's **IO-bound workloads**, and CPU-bound workloads are workloads where those threads are never gonna move from a running into a waiting state because they're not going to be asking for anything IO-related, let's say, to move them into a waiting state. What gets interesting is if we have CPU-bound work and having more threads than there are cores are gonna just cause more pain than good, 'cause a content switch is pure latency, right? But if you have IO-bound work, that means the threads are moving from running states to waiting states to runnable to running to waiting, **and IO-bound work could be making a database** call, disk IO, any sort of thing that would cause the path of execution to have to wait for something. Now, in those scenarios, having more threads per core can be really, really great, because that means that as a thread moves from running to waiting, a core becomes available for more work. And we wanna keep those cores as busy as possible. See, you gotta understand your workload to be able to understand how many threads you can really throw at a problem. You can't just throw an unlimited number of threads, either. And historically, the way we did this for multi-threaded software development was to use **thread pools**. Thread pooling was kind of our way to be able to say, "What is the maximum amount of threads "that give us the best through-put?" And I did a lot of multi-threaded software development on Windows prior to 2013, when I decided, or my company had to move to Linux, and I needed to build programming language for this. And really, Go helped me out with a lot more than just moving to Linux, but when I was on Windows, what we had was a thread-pooling technology called **IOCP**. And Linux has the **epoll** and we got **kevent**, we got other thread pooling tech in the other operating systems. But IOCP, which stands for **IO completion ports**, what Windows solution to thread pooling.



And so if I was writing a multi-threaded software, like a Web service, what I'd have to do is create an IOCP thread pool, where as requests started coming over the wire, we would post those requests into the thread pool and let a family of threads in this pool process it. Here's the problem: how do you know what to size the thread pool, and if you've got certain amounts of cores, we would really wanna make sure that the pool, right, is configured for what the hardware would bear. At the end of the day, what you really need is this. You need a situation where as the number of cores increase, your **through-put** increases, but in a linear fashion like this. 



If you have a situation where you add more cores to an algorithm, and you've got charts that are doing this, you've got a mechanical sympathy problem, right? 

Adding more cores should give us that linear growth, but if not, then we're doing something bad in our algorithms, maybe on our hardware, something bad in our systems. Now this is something that I learned about the Windows operating system. I can't say that it would apply to the other operating systems. But I learned this: let's say I set up this IOCP thread pool. Now the IOCP thread pool had three parameters: min number of threads, let's say we set that up to two, max number of threads, let's say we set that to 24, and had a concurrency value which we always set to zero. This is interesting. It had a concurrency value. Now you set it to zero because what it meant was if there are only, let's say, one core on your system, then only allow one thread to be active at any given time, because if you've got more threads active than cores, you're just adding latency workload. But that always made it interesting to me. Why could I set the min and max values to be larger than the number of cores when I've just said I only wanna use one core, that one thread should only be active for one core, right? Why would I be allowed to use more? Because you've gotta understand your workloads. I keep saying this. So look at what happens in this scenario. Requests start coming in. Let's say this is it, and let's say we are on a two-core, a two-core system. So let's say that we're on a two-core machine, which means that we've got two threads that can run in parallel. I've set that to be two, I've set 24, only two threads can be active. Yeah, so how does this work? Watch this. We turn the server on, requests are coming in, we start throwing them into the pool. Now the very first request that comes in, there are no active threads, so no big deal, the operating system says, "Okay, new thread, "you're going to take it," right? Right now we can have up to two, we start with two, we can have up to 24, and so we say, "You go active." Fine. Now another request comes in, right? So this one here. Another request comes here. Hey, no big deal, you go active. Now, these requests are being processed. Now, a bunch more requests come in, and they're coming into the pool, request, request, request. But guess what? Those two threads are active. We're not allowed to have any more active threads. This work is now piling up. But remember, this is IO-bound work. These threads are probably gonna make a database call. They're, it's a request, databases are involved. And when this makes a request, what's gonna happen is this thread's gonna go from this active state, right, to a waiting state. And now this thread, once it goes into a waiting state, how many threads to we have active? Just one. Okay, we can have up to 24 threads in this pool, create a new thread. And now that we've created this new thread, guess what. Activate it for this piece of work. And now eventually, this thread goes into a waiting state, which means now that that's in a waiting state, we can can add another thread to the pool, and now that we've added another thread to the pool, let's let it do this work. Look, at some point, depending on the amount of time of latency it takes these threads to process the work and the latency on the database, you could end up with 22 threads in here in a waiting state and two threads that are active even though we only have our two-core machine. So this is why you have to understand your workload. And maybe when we have 22 threads active, we've put such a load on the operating system in our work that we actually don't get it done faster. More threads to not mean faster work. So this is the thing that I kind of learned about Windows with these kind of Web request type of software with databases involved. There was this odd, like three to one ratio that I identified. So in other words, if I only had one core, and I never wanted to have have more than three threads in this pool at any given time, because this one to three ratio meant that I could get that work done faster. In other words, if I threw 100,000 requests at the server, and I only had one core, and I used three threads, it got done the fastest. If I used two threads, it was slower because I had more idle CPU time, but if I used four threads it was slower because I had more content switches of latency, I had more work. So if somebody said this wasn't fast enough, you know what I would do, right? What I would do was just say, "Give me another core," which means I can go to six threads. I'd get this linear growth. I could go faster. Hey, that's not fast enough? No problem, give me four cores. I can now go to 12 threads. If I do less or more than 12 on a four-core machine, I'm not going to get the work done faster. This is how these thread pools would help us. We could identify how many cores are on the machine, set up the thread pool based on what we know is our best through-put, and this is how we could do some stuff. But this is cognitive load for us, and we have to run stuff to our servers, we've gotta monitor this, and you know how traffic patterns can change over time, so this isn't really perfect, but it gave us over time a good even through-put, and it allowed us to take advantage of the hardware that was there and try to create this linear growth. Whew! This stuff is complicated, okay? And we would have to do these things regardless of the operating system we're on. But this is one of the things I'm gonna show you that Go is fixing for us in terms of our cognitive load. We still have to understand our workload, but we don't necessarily have to worry about managing our own thread pools anymore. The Go scheduler is going to start doing all of that stuff for us. So now that we have a good understanding of what's happening at the operating system, our threads or our paths of execution, they can be in a running state, a runnable state, which means they're in a run queue, looking to go, or they could be waiting. We understand now that we have to understand our workload. There's CPU-bound workload. There's IO-bound workload. If you want less is more with the threads, then all things are gonna dictate how we architect, design, and throw work at our machine to get it done as fast as possible. So the next thing that we've gotta learn now that we kind of looked at the operating system is how the Go scheduler works, which sits on top of it.