

Now, I showed you a little bit of this when we're talking about the garbage collector. We're gonna deep into now how the Go scheduler works and sits on top of the operating system. So, when your Go program starts up, it is given a **logical processor** that we call P. This is modeled very much after the operating, something like the Linux operating system and the things that I showed you. That **logical processor P** is given an M that stands for **machine**, which represents **a real-life operating system thread or the operating system path of execution** which the operating system is still responsible to schedule on some **core**, everything that we just talked about. 



Okay. Then there's two data structures here. There's the **global run queue**, and every P has a **local run queue.** Remember we talked about run queues are the core, the processor level within the operating system. Because **a goroutine just like a thread is a path of execution**, which mans that goroutine also can be in one of three states. **Running**, **runnable**, or **waiting**. It's all the same stuff. So, when a goroutine, like the main goroutine when your Go program starts up, you get the main goroutine, right? We could show that the main goroutine is executing on our context right here. Okay, there it is. We might even just maybe wanna draw it like this to make it a little bit more clear that our goroutine, our main goroutine is now executing on the M, on the context, because it belongs to this P. And maybe this main goroutine goes ahead and creates a couple more goroutines. So, those goroutines end up finding themselves in a local run queue because they're in a runnable state. This is executing, there it is. These are in a runnable state, there they are. Now, sometimes a goroutine that's in a runnable state might find itself in the global run queue because a P hasn't taken it yet. This is a work stealing scheduler. We'll talk a little bit more about that. So, goroutines could end up in the global run queue. This P pulls in and says, "Hey, I'm stealing this work. "I'm gonna take it." Ends up in the local run queue right here, and you got this go routine, and it's executing a runnable, a running state right there. So, this is it. And if you've got multiple Ps because I've got multiple cores, we're only gonna get a single P or thread per core. Think about what that decision is. What we're saying is that we don't need more threads than we have cores, because we don't wanna put more load on the operating system. We at the Go level will manage these paths of execution, and I'm gonna show you where all those sympathies are. But if we have this situation, then you could have two goroutines running in parallel, and you could have goroutines in the local run queues across these Ps. So, you're starting to see this. Now, let's just start here with some basic scheduling. The Go scheduler runs up in our user mode or in our user space. So, understand that the processor can be switched in two different modes. There's two classic modes we talk about all the time, kernel mode and user mode. And kernel mode basically tells the processor that whatever code is currently executing, it can do whatever, whatever it wants. So, when you're executing operating system code, like you make a system call and now the operating system servicing that and executing on the core, that process switch to kernel mode, and this is why drivers can cause havoc or bring operating systems down because the drivers running kernel mode. Kernel mode means this code can do whatever it wants. They'll just let it go. But when we're executing our code, our code here, our Go code, it's executing in user mode, which puts the processor in a protected mode. And it tells the processor, "Look, this code is not allowed to do whatever it wants, "and if it's about to do something we don't like, "then fault the application to protect this house." Since the Go scheduler is built into the Go runtime and the Go runtime is built in to your Go application, it means the scheduler runs up in user mode. And since the scheduler runs in user mode, we now have to say that the Go scheduler is not a preemptive scheduler, but a cooperating scheduler. Now that's really like a four-letter word for us dinosaurs who code it on cooperating schedulers 20-plus years ago. Because cooperating schedulers mean that the application developer has to do the cooperation. In other words, if a piece of code got time on that core through thread, then the application developer has to cooperate and say, "Okay, I'm willing to give up my time slice. "I'm willing to give up my time slice, I'm willing." But the reality is if I'm an application developer and I get a chance for my code to run on that core, I ain't giving it up, and we didn't. We were very, very selfish. So, when I think about a cooperating schedule, I start to cringe because there's really a lot of good concurrency around it, because we're asking developers to make decision. But Go has kinda change the game here with cooperating scheduler, because it's not the developer who is having to cooperate. **The Go scheduler is doing all the cooperation.** Because of that, what's brilliant is this user space, user space cooperating scheduler looks and feels preemptive, just like the operating system scheduler. And so, I want us to put into our head that the Go scheduler is a preemptive scheduler because when all things are equal, you have no clue what the scheduler is going to do. It is as undeterministic as that operating system scheduler is. So, technically cooperating, but it has this semantics of a preemptive scheduler. 

And when all things are equal, it is undeterministic, and we do not write code with the idea that we're gonna predict what the scheduler is gonna do. You have to to control it, and that's when we start talking about synchronization and orchestration later. So, this is our model. Now, for the runtime scheduler to make a scheduling decision, a cooperating scheduling decision, the events have to occur in the code. And these events happen through function calls today. They are looking in 1.11, which is in beta right now as I speak, to find statements that can also be used to help make scheduling decision where we're not gonna be stuck in tight moves where function calls aren't gonna be made. But at least understand how complex it is to be able to do statement level context switching like that, because things have to be atomic. And the runtime and the schedule has to guarantee that statements are atomic. It's very complicated stuff, amazing, stuff that I can never work on, but some of it is coming. 

So, as of 1.10 today, everything happens through **function call.** Now there's three classes of events that are gonna allow the scheduler to make a scheduling decision. There is the use of the keyword **go**, and this is how we create goroutine. We're gonna add a new path of execution. Any function or method in our code can become a goroutine or a path of execution by using the keyword go in front of the function call. 

We also have garbage collection. Anytime garbage collection kicks in, there's gonna be a lot of scheduling decisions being made because the garbage collection goroutines also have to use the Ps that are available, a lot of chaos. ***Number two***, **system calls**, system calls. There are system calls happening all the time. Anytime you call log or font.print, that's a system call. So there's lots of availability of the scheduler to make these cooperative scheduling decision. One, two, three. And really let's just put it into a list of **blocking calls**. So, anything that could cause that goroutine to have to go into a waiting state, like the use of a **mutex** or an atomic instruction, something like that, look, we're gonna end up having, and these things are happening all the time. And therefore, the scheduler even though it's cooperating can look and feel very, very **preemptive**. But let's talk about system calls because this is really interesting. 



We are fortunate today to, for the majority of us that are production system operating systems, have the ability to do asynchronous system calls. And what we try to do in Go is leverage these asynchronous system calls for as many things as we can. We use it today for a lot of our networking calls. We use it today for a lot of disk I/O type of calls. We try to leverage this as much as possible. And this is kind of why. Let's say that this goroutine right here wants to make a system call. Remember we're on a single-threaded Go program. Anytime we have one P, a single-threaded Go program. This goroutine wants to make a system call, wants to open up a file. How long can that take? Dude, that could take seconds. It could take eternity. So if we allow this goroutine to block this operating system thread, then for that amount of time of latency, we're not getting any other work done. We're basically in a **stop the world situation**. We can't have that. So, what Go has is a really special data structure here, a system that we call the **network poller**, and this is leveraging that thread pooling technology. So, in Windows it's using IOCP, kevent or epoll on Linux, and this has a thread. And what we do is when that goroutine wants to make a system call, well, then what happens is this goroutine is context switched off of the M, there it is. It's placed over here in the network poller, There it is, it's now in a waiting state, and its request, its operating system of corse gets posted into the network poller, and we handle the system call asynchronously. This is great because we just freed up the M to do more work, and that's exactly what's gonna happen. This goroutine that's in a runnable state now moves into an execution state. It's running now, it's runnable. Now, again, when these goroutines are, where now things are equal, we don't know which one it's going to choose, it's choosing that one. So, look at what we've done here. Even though we are single-threaded Go program, this thread here part of the network poller is now servicing the asynchronous system call. Once the net poller comes back or the operating system comes back with the system call, then we can take this goroutine and move it back into the local run queue where it sits back into a runnable state. And eventually, what we'll end up doing is another context switch, and maybe this main goroutine now comes back and is executing. This goes back into a runnable state. How cool is that? The concepts of an asynchronous system call at the operating system level really give us high levels of efficiency here because we can still just really maintain a small number of threads for entire Go program, one thread per core, and this thread for our network poller. Again, if that goroutine is gonna be doing any sort of networking, it's gonna move here. So, anything we can do asynchronously to keep this thread busy is what we're gonna wanna do. Now, there may be times when you're on an operating system that doesn't support asynchronous system calls where there's some system call that we can't deal with asynchronously, or you're dealing with Cgo where you got a C library that no matter what we do, we're gonna be blocking that thread. When that happens, the following thing happens with the scheduler. Now, we don't want this to happen, and most of what you're doing won't do this, but watch what happens. This goroutine wants to make a call. It's now gonna block this M, there's no way to do it asynchronously, there's no way to move it to the network poller. So, what the schedule is gonna do is gonna detach that M, there it is, and the G. It's gonna detach it off the P, let it block. This is blocking calls right here. So, we're now blocking that M, and what that Go schedule is gonna do now, this is done, is kinda come in and bring a new threading. Here's M2. We now have an extra thread in our Go program. But guess what? We're still a single-threaded Go application, because now that goroutine is in that executing state, there it is. And we're still processing the run queue even though that other thread out here is blocked. Now, by default, in this scenario, you can have up to 10,000. You can have up to 10,000 of these threads before you go program faults. That really shouldn't be happening. If that happens to you, you reach out to me. I wanna see that. And maybe about once a year, I see it on the mailing list. But eventually, the blocking call is done. The goroutine is over here. Our main goroutine goes back into a runnable state, and then this thread is put on the side for later use in case it happens again. Again, this is not the normal scenario. The normal scenario is we're using the network poller that handle asynchronous system calls to keep that just one thread in our system. This is the situation where the schedule identified that that M is going to block, in which case we've got to replace it to keep things going. Very, very, very cool stuff. And like I told you before that this is a work still in scheduler. So, the other idea is that if we are in a multi-P environment, we're in a multi-P environment, and let's say this P now has no more work. One of the first things this P is going to do is look in the global run queue to take in some work if found that it's now executing this. It might even decide, hey, you know what, you're overloaded over here with goroutine in a runnable state. I've got no work to do. I'm taking this go routine now and I'm gonna put it here. There is a special stack in the scheduler trace that shows when a M on a P is spinning. We call that spinning. So, anytime in the scheduler an M doesn't have goroutine work to do, but it's looking for goroutines, then that M is gonna be spinning. Ideally, we wanna keep the M busy at all times, because we don't want from the operating system point of view the operating system to say, "Hey, there's an M on my core "and it just went from an executing into a wait state. "Let's pull it off in context switch." The more we can keep these Ms busy, the more time they get, the more work we can get done. This is one of the mechanical sympathies here around the Go scheduler. The idea is that we wanna keep these threads busy. I wanna show you a scenario here that hopefully should bring this home for us. Imagine we were writing a traditional multi-threaded piece of software. Let's say we're using C, we're using real operating system threads. Now let's say that even if we're on a multi-core machine, here's thread one, here's thread two. Even we're on a multi-core machine, and these threads could run on their own core, let's say that these two threads need to pass messages back and forth. This is I/O bound work. Okay, no problem. So, what we're gonna do is wait for a context switch, we get it, we're now executing on the core, we do a little work, we send the message over here. And once we send the message, we're gonna have another context switch because during this transition we're gonna go into a waiting state. We're now waiting. Now we're waiting for a context switch because this thread was in a wait state. It now has signaled somehow that it's got a message waiting for it. so, we get a context switch, there it is. It can now get the message, process it. It sends the message over. Now we get another context switch, another context switch, context switch, context switch. Every time these two threads wanna pass a message or do any sort of orchestration, we're going form waiting to runnable, to running, to waiting, to runnable, and these threads are coming on and off this course. Remember I told you this context switch is very, very expensive. But imagine that we're not dealing with operating system threads directly. Let's say that we're in Go. Let's say we're working with goroutines. In fact, let's say that we decided that we're gonna run on a single P. Let's say that we decided what we're gonna do is run on a single P. We're single threaded. Now, this chart doesn't change. We need a context switch to run, but our context switch is happening on the M which also is super lightweight, because the context switch in Go is faster because the scheduler knows what the goroutine is doing, and therefore it can save a lot less state than the operating system does. So context goroutines are not just light weight, but they're also much faster in terms of their context switch on the M. So, look, we get a context switch no problem. Now this comes off, now this comes on, now this comes off, now this comes on, now this comes off, but we still have all of that same sort of kinda I/O happening with the goroutine. But what I really want you to understand is this. During every single context switch here in Go, in our application, from the operating system perspective, that thread never went into a waiting state. From the operating system's perspective, that thread is always busy. It's always I a running or a runnable state. What Go has done, what the brilliance is of this scheduler is Go has turned I/O bound work into CPU bound work. When you have CPU bound work, more threads than cores can only add load because we don't need the extra context, which is those corse will never be idle to do other work because this thread has always got work to do. Think about it. Go has turned I/O bound work into CPU bound work. Oh my God, how brilliant is that? It really is, it's just this brilliant piece of engineering. And what we're gonna find out is that this P also takes a huge amount of cognitive load off of us because I don't have to think about thread pools, or goroutine pools anymore to figure out throughput and efficiency. In many cases, we can do one of two things. We could just throw a lot of work at the scheduler and let it distribute that across our Ps, and do that efficiently. We can still take the ideas that let's just leverage a small amount of goroutine, maybe one per P, and keep them busy on that. There's tons of little things we can do. We'll talk about this stuff. What I really wanted to show you here, how the mechanical sympathies of the Go scheduler, how it sits on top of the operating system. We don't wanna rewrite the operating system schedulers. Schedulers are complex. They understand hardware, they understand how the hardware is laid out, they understand NUMA, they understand a bunch of things. And if the Go scheduler is able to sit here, and again the real magic is that the Go scheduler turns I/O bound work into CPU bound work which means it minimizes and reduces a full amount of load off the OS. And we manage our concurrency and parallelism here in Go.





我们不希望这样的事情发生 你所做的大部分事情都不会这样做 但你要看清楚了 这个goroutine想做一个调用。它现在要阻止这个M，没有办法异步地做，没有办法把它移到网络轮询器上。所以，日程表要做的就是把M和G分离出来，把它从P上分离出来，让它阻塞。这是在阻断电话 所以，我们现在阻断了M，而那个Go计划表现在要做的是，这已经完成了，是有点进来并带来一个新的线程。这里是M2. 我们现在有一个额外的线程在我们的围棋程序中。但你猜怎么着？我们仍然是一个单线程的Go程序，因为现在那个goroutine处于执行状态，就在那里。而且我们仍然在处理运行队列，即使外面那个线程被阻塞了。现在，默认情况下，在这种情况下，你可以有多达10,000个线程。在程序出错之前，你最多可以拥有10000个这样的线程。这真的不应该发生。如果你遇到这种情况，你就来找我。我想看到的。也许每年有一次，我在邮件列表上看到它。但最终，屏蔽电话已经完成了。Goroutine是在这里。我们的主goroutine又回到了可运行的状态，然后这个线程被放在一边，以便以后使用，以防再次发生。同样，这不是正常的情况。正常的情况是我们使用处理异步系统调用的网络轮询器来保持系统中只有这一个线程。这种情况下，计划表确定那个M要阻塞了，在这种情况下，我们要替换掉它来维持事情的进行。非常、非常、非常酷的东西。就像我之前跟你说的，这是个还在调度器中的工作。所以，另外一个想法是，如果我们在一个多P的环境下，我们在一个多P的环境下，我们说这个P现在没有工作了。这个P首先要做的一件事情就是在全局运行队列中寻找一些工作，如果发现它现在正在执行这个。它甚至可能会决定，嘿，你知道吗，你这里超载了，goroutine处于可运行状态。我没有工作可做了。我现在要把这个go例程放在这里。调度器跟踪中有一个特殊的堆栈 可以显示P上的M在旋转时的情况。我们称之为 "旋转 所以，在调度器中，只要一个M没有goroutine工作要做，但它在寻找goroutine，那么这个M就会旋转。理想情况下，我们希望M一直处于忙碌状态，因为我们不希望从操作系统的角度来看，操作系统说："嘿，我的核心上有一个M，它刚刚从执行状态变成了等待状态" "让我们在上下文切换中把它拉下来"。我们越是能让这些Ms忙起来，他们的时间越多，我们能完成的工作就越多。这就是围棋调度器的机械同情心之一。我们的想法是，我们想让这些线程忙起来。我想给你看一个场景，希望能让你明白这个道理。想象一下，我们正在编写一个传统的多线程软件。假设我们使用的是C语言，我们使用的是真正的操作系统线程。现在我们说，即使我们在多核机器上，这里是线程一，这里是线程二。即使我们在多核机器上，这些线程也可以在自己的核心上运行，比方说这两个线程需要来回传递消息。这是I/O绑定的工作。好的，没问题。所以，我们要做的是等待一个上下文切换，我们得到它，我们现在在核心上执行，我们做一点工作，我们把消息发送到这里。一旦我们发送了消息，我们就会有另一个上下文切换，因为在这个过渡期间，我们会进入一个等待状态。我们现在是在等待。现在我们在等待上下文切换，因为这个线程处于等待状态。现在，它已经发出信号，表明有一条消息在等着它，所以，我们得到一个上下文切换，就在那里。它现在可以得到消息，处理它。它把信息发送过去。现在我们得到另一个上下文切换，另一个上下文切换，上下文切换，上下文切换。每当这两个线程想要传递消息或者进行任何形式的协调时，我们就会从等待到可运行，再到运行，再到等待，再到可运行，这些线程在这个过程中来来去去。记得我告诉过你这个上下文切换是非常非常昂贵的。但是想象一下，我们不是直接和操作系统的线程打交道。比方说，我们在围棋中。比方说我们在和goroutines一起工作。事实上，假设我们决定要在一个P上运行，假设我们决定要做的是在一个P上运行，我们是单线程。现在，这个图表不会改变。我们需要一个上下文切换来运行，但我们的上下文切换发生在M上，这也是超级轻量级的，因为在Go中上下文切换更快，因为调度器知道goroutine在做什么，因此它可以比操作系统节省很多状态。所以